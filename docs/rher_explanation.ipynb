{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac0147b",
   "metadata": {},
   "source": [
    "# Gi·∫£i th√≠ch file `drl/learning/rher.py`\n",
    "\n",
    "T√†i li·ªáu n√†y t·ªïng h·ª£p c√°c th√†nh ph·∫ßn ch√≠nh c·ªßa `drl/learning/rher.py` (phi√™n b·∫£n trong workspace).\n",
    "\n",
    "## M·ª•c ƒë√≠ch\n",
    "- `RHER` l√† m·ªôt framework hu·∫•n luy·ªán d√πng Hindsight Experience Replay cho b√†i to√°n nhi·ªÅu giai ƒëo·∫°n (multi-stage). L·ªõp `RHER` k·∫ø th·ª´a `RLFrameWork` v√† qu·∫£n l√Ω lu·ªìng: t·∫°o rollout, l∆∞u v√†o b·ªô nh·ªõ (memory), hu·∫•n luy·ªán agent v√† ƒë√°nh gi√°.\n",
    "\n",
    "## C√°c import ch√≠nh\n",
    "- `torch`, `numpy`, `tqdm`, `SummaryWriter` ‚Äî th∆∞ vi·ªán ML, x·ª≠ l√Ω tensor v√† ghi log TensorBoard.\n",
    "- T·ª´ d·ª± √°n: `RLFrameWork`, `map_structure`, `put_structure`, `groupby_structure`, `nearest_node_value`, `MeanMetrics`, `LOGGER`, `format_time`, `format_tabulate` ‚Äî c√°c ti·ªán √≠ch v√† ki·ªÉu c∆° s·ªü.\n",
    "\n",
    "## Ki·∫øn tr√∫c l·ªõp `RHER`\n",
    "- `__init__(envs, agent, memory, compute_metrics=None)`\n",
    "  - Ki·ªÉm tra interface c·ªßa `agent` v√† `memory` (v√≠ d·ª• `memory.reward_func`, `memory.num_stages`, `memory.horizon`).\n",
    "  - `compute_metrics` m·∫∑c ƒë·ªãnh: k·∫øt h·ª£p `goal_achieved` v√† t·ªâ l·ªá reward/horizon ƒë·ªÉ t·∫°o `eval_value` t·ªïng h·ª£p.\n",
    "\n",
    "### Exploration / stage handling\n",
    "- `_get_stages(observations, mt_goals)`\n",
    "  - G·ªçi `memory.reward_func` cho t·ª´ng `stage_id` ƒë·ªÉ nh·∫≠n `goal_achieved` t·∫°i m·ªçi stage.\n",
    "  - T√≠nh `stages` hi·ªán t·∫°i cho m·ªói ph·∫ßn t·ª≠ trong batch b·∫±ng c√°ch x√©t stage l·ªõn nh·∫•t ƒë√£ ƒë·∫°t.\n",
    "\n",
    "- `_select_action(observations, mt_goals, stages, *, r_mix=0.5)`\n",
    "  - `r_mix` (0..1) l√† x√°c su·∫•t ƒë·ªÉ \"tr·ªôn\" goal: v·ªõi x√°c su·∫•t `r_mix` m·∫´u s·∫Ω ƒë∆∞·ª£c chuy·ªÉn t·ª´ stage hi·ªán t·∫°i sang stage k·∫ø ti·∫øp tr∆∞·ªõc khi ch·ªçn action.\n",
    "  - M·ª•c ƒë√≠ch: h∆∞·ªõng d·∫´n exploration theo m·ª•c ti√™u (goal-directed exploration) b·∫±ng c√°ch ƒë√¥i khi cung c·∫•p goal c·ªßa stage ti·∫øp theo.\n",
    "  - G·ªçi `agent({'observation':..., 'goal':..., 'task_id': stages}, deterministic=False)` ƒë·ªÉ l·∫•y action non-deterministic.\n",
    "\n",
    "- `select_actions(..., deterministic=False)`\n",
    "  - N·∫øu kh√¥ng deterministic: chu·∫©n ho√° d·ªØ li·ªáu b·∫±ng `agent.format_data`, g·ªçi `_get_stages` v√† `_select_action`, tr·∫£ v·ªÅ `(actions_numpy, goal_achieveds_bool)`.\n",
    "  - N·∫øu deterministic: d√πng goal cu·ªëi c√πng v√† stage cu·ªëi, g·ªçi policy deterministic.\n",
    "\n",
    "### Training\n",
    "- `train(num_updates, batch_size, future_p=0.8, n_steps=1, step_decay=0.7, discounted_factor=0.99, clip_return=None)`\n",
    "  - N·∫øu d√πng prioritized replay th√¨ c·∫≠p nh·∫≠t priorities tr∆∞·ªõc.\n",
    "  - L·∫∑p `num_updates` l·∫ßn: sample t·ª´ `memory.sample(...)` (·ªü ƒë√¢y HER sampling ƒë∆∞·ª£c x·ª≠ l√Ω trong memory), g·ªçi `agent.update(...)` ƒë·ªÉ c·∫≠p nh·∫≠t m·∫°ng.\n",
    "  - Tr·∫£ v·ªÅ `MeanMetrics` ch·ª©a c√°c metric trung b√¨nh.\n",
    "\n",
    "### Evaluate\n",
    "- `evaluate(num_episodes=10)`\n",
    "  - Ch·∫°y envs ·ªü ch·∫ø ƒë·ªô `eval`, ch·ªçn action deterministic, thu th√¥ng tin episode (reward, horizon, infos), s·ª≠ d·ª•ng `compute_metrics` ƒë·ªÉ t√≠nh `eval_value`.\n",
    "\n",
    "### Run (lu·ªìng ch√≠nh)\n",
    "- `run(...)`\n",
    "  - T·∫°o th∆∞ m·ª•c experiment (`make_exp_dir`), c√≥ th·ªÉ `resume` t·ª´ checkpoint.\n",
    "  - V·ªõi m·ªói epoch: l·∫∑p cycles, m·ªói cycle g·ªçi `generate_rollouts`, `store_rollouts`, sau ƒë√≥ `train` nhi·ªÅu l·∫ßn.\n",
    "  - Sau epoch ch·∫°y `evaluate` v√† l∆∞u policy/ckpt n·∫øu c·∫ßn.\n",
    "\n",
    "### Rollout / Memory\n",
    "- `generate_rollouts(r_mix='auto')`\n",
    "  - T·∫°o rollout theo `memory.horizon`. M·ªói step: g·ªçi `select_actions` (non-deterministic), th·ª±c hi·ªán `envs.step`, thu observations, action, v√† `goal_achieved`.\n",
    "  - Tr·∫£ v·ªÅ batch rollouts (c·∫•u tr√∫c ƒë√£ swap axis) v√† `info` (mean eps_reward, eps_horizon, ...).\n",
    "\n",
    "- `store_rollouts(rollouts)`\n",
    "  - Chu·∫©n ho√° c√°c chi·ªÅu m·∫£ng (`meta`, `achieved_goal`, `desired_goal`) tr∆∞·ªõc khi g·ªçi `memory.store(rollouts)`.\n",
    "\n",
    "## V·ªÅ `r_mix` v√† entropy\n",
    "  - `r_mix`: exploration theo goal (thay ƒë·ªïi input m·ª•c ti√™u ƒë·ªÉ agent d·∫´n h∆∞·ªõng t·ªõi stage ti·∫øp theo).\n",
    "  - Entropy: exploration ·ªü action-space (policy th·ª≠ nhi·ªÅu action kh√°c nhau cho c√πng goal).\n",
    "\n",
    "## V√≠ d·ª• c·ª• th·ªÉ v·ªÅ `r_mix` v√† entropy trong hu·∫•n luy·ªán c√°nh tay robot\n",
    "\n",
    "### B·ªëi c·∫£nh m·∫´u\n",
    "- Task: pick-and-place chia th√†nh 4 stage: `reach` ‚Üí `grasp` ‚Üí `lift` ‚Üí `place`.\n",
    "- `memory.num_stages = 4`, m·ªói rollout c√≥ `memory.horizon` b∆∞·ªõc.\n",
    "- Agent c√≥ th·ªÉ l√† SAC (policy Gaussian, c√≥ h·ªá s·ªë entropy `alpha`) ho·∫∑c DDPG/TD3 (th√™m noise v·ªõi std).\n",
    "\n",
    "### √ù t∆∞·ªüng ch√≠nh (t√≥m t·∫Øt)\n",
    "- `r_mix`: x√°c su·∫•t t·∫°i m·ªói b∆∞·ªõc ƒë·ªÉ thay goal hi·ªán t·∫°i b·∫±ng goal c·ªßa stage ti·∫øp theo tr∆∞·ªõc khi policy ch·ªçn action (goal-guided exploration).\n",
    "- Entropy / noise: ƒëi·ªÅu ch·ªânh m·ª©c ƒë·ªô ng·∫´u nhi√™n c·ªßa policy khi ch·ªçn action cho c√πng m·ªôt (state, goal) (action-space exploration).\n",
    "\n",
    "### C·∫•u h√¨nh m·∫´u v√† h√†nh vi mong ƒë·ª£i\n",
    "- C·∫•u h√¨nh A ‚Äî Th·∫≠n tr·ªçng:\n",
    "  - `r_mix = 0.0`, entropy th·∫•p / noise nh·ªè (SAC `alpha ‚âà 0.01`, DDPG noise std ‚âà 0.02).\n",
    "  - Agent h·ªçc ch·∫Øc t·ª´ng stage, √≠t th·ª≠ nghi·ªám goal k·∫ø ti·∫øp, ·ªïn ƒë·ªãnh nh∆∞ng c√≥ th·ªÉ ch·∫≠m qua stage.\n",
    "\n",
    "- C·∫•u h√¨nh B ‚Äî C√¢n b·∫±ng:\n",
    "  - `r_mix = 0.5`, entropy moderate (SAC `alpha ‚âà 0.05 - 0.2`, DDPG std ‚âà 0.05 - 0.15).\n",
    "  - M·ªôt ph·∫ßn rollout h∆∞·ªõng t·ªõi m·ª•c ti√™u ti·∫øp theo, policy v·∫´n th·ª≠ nhi·ªÅu h√†nh ƒë·ªông ‚Äî c√≥ l·ª£i cho chuy·ªÉn ti·∫øp gi·ªØa stage.\n",
    "\n",
    "- C·∫•u h√¨nh C ‚Äî H∆∞·ªõng m·∫°nh m·ª•c ti√™u ti·∫øp theo:\n",
    "  - `r_mix = 1.0`, entropy th·∫•p (n·∫øu entropy cao ƒë·ªìng th·ªùi c√≥ th·ªÉ g√¢y b·∫•t ·ªïn).\n",
    "  - Agent li√™n t·ª•c nh·∫≠n goal c·ªßa stage ti·∫øp theo, ƒë·∫©y nhanh h·ªçc chuy·ªÉn giai ƒëo·∫°n nh∆∞ng c√≥ r·ªßi ro n·∫øu k·ªπ nƒÉng c∆° b·∫£n y·∫øu.\n",
    "\n",
    "- Th·ª≠ nghi·ªám kh√°m ph√° r·ªông:\n",
    "  - `r_mix = 0.8` k·∫øt h·ª£p entropy cao (SAC `alpha` l·ªõn ho·∫∑c noise std cao) ‚Üí kh√°m ph√° r·∫•t r·ªông, kh√≥ h·ªôi t·ª• nh∆∞ng c√≥ th·ªÉ t√¨m chu·ªói h√†nh ƒë·ªông ph·ª©c t·∫°p.\n",
    "\n",
    "### V√≠ d·ª• s·ªë cho pick-and-place\n",
    "- Setup: `batch_size=256`, `horizon=50`, `num_stages=4`.\n",
    "- Grid th·ª≠: `r_mix` ‚àà {0.0, 0.25, 0.5, 0.75, 1.0} √ó SAC `alpha` ‚àà {0.01, 0.05, 0.2} (ho·∫∑c DDPG std ‚àà {0.02, 0.08, 0.2}).\n",
    "- Theo d√µi: success rate per stage, time-to-first-success, average reward, variance. Ch·∫°y nhi·ªÅu seed ƒë·ªÉ ki·ªÉm nghi·ªám.\n",
    "\n",
    "### C√°ch c·∫•u h√¨nh trong code\n",
    "- Truy·ªÅn `r_mix` khi g·ªçi `run()`:\n",
    "\n",
    "```python\n",
    "# v√≠ d·ª•\n",
    "rher.run(..., r_mix=0.5, num_updates=50, batch_size=256)\n",
    "```\n",
    "\n",
    "- C·∫•u h√¨nh entropy cho SAC (v√≠ d·ª•):\n",
    "\n",
    "```python\n",
    "# n·∫øu agent l√† SAC\n",
    "sac_agent = SACAgent(..., entropy_coef=0.05)\n",
    "```\n",
    "\n",
    "- C·∫•u h√¨nh noise cho DDPG:\n",
    "\n",
    "```python\n",
    "ddpg_agent = DDPGAgent(..., action_noise_std=0.08)\n",
    "```\n",
    "\n",
    "### L∆∞u √Ω tuning & th·ª≠ nghi·ªám ƒë·ªÅ xu·∫•t\n",
    "- B·∫Øt ƒë·∫ßu v·ªõi `r_mix=0.25-0.5` v√† entropy moderate (SAC alpha auto ho·∫∑c ~0.05). Quan s√°t: n·∫øu agent kh√¥ng ti·∫øn qua stage 1‚Üí2 th√¨ tƒÉng `r_mix`; n·∫øu agent th·∫•t b·∫°i nhi·ªÅu ·ªü stage hi·ªán t·∫°i th√¨ gi·∫£m `r_mix` ho·∫∑c gi·∫£m entropy.\n",
    "- Th·ª≠ m·ªôt bi·∫øn t·∫°i m·ªôt th·ªùi ƒëi·ªÉm (ch·ªâ ƒë·ªïi `r_mix` ho·∫∑c ch·ªâ ƒë·ªïi entropy`) ƒë·ªÉ th·∫•y t√°c ƒë·ªông r√µ r√†ng.\n",
    "- Th·ª≠ nghi·ªám ƒë·ªÅ xu·∫•t (ng·∫Øn):\n",
    "  1. Baseline: `r_mix=0.0`, SAC `alpha=0.05`.\n",
    "  2. Th√™m `r_mix=0.5`, gi·ªØ `alpha` c·ªë ƒë·ªãnh ‚Äî so s√°nh success per stage.\n",
    "  3. Gi·ªØ `r_mix=0.5`, thay ƒë·ªïi `alpha` l√™n/xu·ªëng.\n",
    "\n",
    "### C·∫£nh b√°o\n",
    "- `generate_rollouts` trong repo c√≥ default `r_mix='auto'` nh∆∞ng `_select_action` mong `r_mix` l√† s·ªë trong `[0,1]`. H√£y ƒë·∫£m b·∫£o khi g·ªçi `generate_rollouts()` truy·ªÅn gi√° tr·ªã s·ªë th·ª±c ho·∫∑c s·ª≠a `generate_rollouts` ƒë·ªÉ x·ª≠ l√Ω `'auto'` h·ª£p l√Ω.\n",
    "\n",
    "## L∆∞u √Ω th·ª±c d·ª•ng\n",
    "- `memory.reward_func` ph·∫£i h·ªó tr·ª£ `stage_id` (multi-stage reward).\n",
    "- `generate_rollouts` m·∫∑c ƒë·ªãnh `r_mix='auto'` nh∆∞ng `_select_action` c·∫ßn `r_mix` l√† s·ªë trong `[0,1]`. N·∫øu g·ªçi `generate_rollouts()` m√† kh√¥ng truy·ªÅn `r_mix` th√¨ `'auto'` c√≥ th·ªÉ g√¢y l·ªói. N√™n s·ª≠a ƒë·ªÉ x·ª≠ l√Ω `'auto'` ho·∫∑c lu√¥n truy·ªÅn `r_mix` t·ª´ `run()`.\n",
    "- `agent` ph·∫£i cung c·∫•p `format_data`, `device`, `update`, v√† ch·∫•p nh·∫≠n input `{'observation','goal','task_id'}`.\n",
    "- `memory` ph·∫£i implement `store`, `sample`, `reward_func`, `num_stages`, `horizon`, v√† n·∫øu d√πng prioritized replay th√¨ `use_priority` + `update_priorities()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398add0",
   "metadata": {},
   "source": [
    "## üìö Gi·∫£i th√≠ch chi ti·∫øt v·ªÅ Reuse Ratio\n",
    "\n",
    "### **1. Reuse Ratio l√† g√¨?**\n",
    "\n",
    "**Reuse Ratio** (T·ª∑ l·ªá t√°i s·ª≠ d·ª•ng) l√† s·ªë l·∫ßn trung b√¨nh m·ªói sample trong replay buffer ƒë∆∞·ª£c s·ª≠ d·ª•ng l·∫°i cho training trong m·ªôt kho·∫£ng th·ªùi gian (th∆∞·ªùng l√† 1 epoch).\n",
    "\n",
    "**C√¥ng th·ª©c:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52097981",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Reuse Ratio = Total Samples Consumed / New Samples Collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e1e00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **2. T√≠nh to√°n cho c·∫•u h√¨nh c·ªßa b·∫°n:**\n",
    "\n",
    "#### **Thu th·∫≠p d·ªØ li·ªáu (Data Collection):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 32\n",
    "num_cycles = 800  \n",
    "# ‚Üí S·ªë episodes thu th·∫≠p m·ªói epoch = 800 episodes\n",
    "\n",
    "horizon = 128 steps/episode\n",
    "# ‚Üí Transitions m·ªõi m·ªói epoch = 800 √ó 128 = 102,400 transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d68210",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Ti√™u th·ª• d·ªØ li·ªáu (Data Consumption):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_updates = 256 per cycle\n",
    "cycles_per_epoch = 800 / 32 = 25 batches\n",
    "\n",
    "# T·ªïng updates m·ªói epoch:\n",
    "total_updates = 256 √ó 25 = 6,400 updates\n",
    "\n",
    "# T·ªïng samples ti√™u th·ª• m·ªói epoch:\n",
    "total_samples = 512 √ó 6,400 = 3,276,800 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772bc49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Reuse Ratio:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bde9fd",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Reuse Ratio = 3,276,800 / 102,400 ‚âà 32√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4ed1b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Ch√≠nh x√°c h∆°n v·ªõi RHER:**\n",
    "- RHER sample t·ª´ multiple stages (num_stages=2)\n",
    "- M·ªói stage sample ri√™ng: `3,276,800 / 2 = 1,638,400 samples/stage`\n",
    "- Nh∆∞ng v·∫´n t·ª´ c√πng pool 102,400 transitions\n",
    "- **Effective Reuse Ratio ‚âà 32√ó** (kh√¥ng ph·∫£i 4096√ó nh∆∞ t√¥i t√≠nh nh·∫ßm tr∆∞·ªõc!)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. √ù nghƒ©a c·ªßa Reuse Ratio:**\n",
    "\n",
    "#### **üîÑ Reuse Ratio = 1√ó (Ideal On-Policy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4dba3e",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Collection:  100 samples\n",
    "Consumption: 100 samples (s·ª≠ d·ª•ng ƒë√∫ng 1 l·∫ßn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295973f",
   "metadata": {},
   "source": [
    "- **V√≠ d·ª•**: PPO, TRPO (on-policy algorithms)\n",
    "- M·ªói sample ch·ªâ d√πng 1 l·∫ßn r·ªìi b·ªè\n",
    "- Sample efficiency th·∫•p nh∆∞ng tr√°nh overfitting\n",
    "\n",
    "#### **üîÑ Reuse Ratio = 10-50√ó (Normal Off-Policy)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661b2cf",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Collection:  100 samples/epoch\n",
    "Consumption: 1,000-5,000 samples/epoch\n",
    "Reuse:       10-50√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c6311",
   "metadata": {},
   "source": [
    "- **V√≠ d·ª•**: SAC, DDPG v·ªõi replay buffer ti√™u chu·∫©n\n",
    "- Sample nhi·ªÅu l·∫ßn t·ª´ buffer l·ªõn\n",
    "- C√¢n b·∫±ng gi·ªØa sample efficiency v√† stability\n",
    "\n",
    "#### **üîÑ Reuse Ratio = 100-1000√ó (High Reuse)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea12a4",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Collection:  100 samples/epoch  \n",
    "Consumption: 10,000-100,000 samples/epoch\n",
    "Reuse:       100-1000√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3fe39",
   "metadata": {},
   "source": [
    "- Th∆∞·ªùng g·∫∑p khi:\n",
    "  - Buffer l·ªõn, collect √≠t\n",
    "  - Nhi·ªÅu updates per cycle\n",
    "  - Multi-stage sampling (nh∆∞ RHER)\n",
    "- **Risk**: Overfitting to replay buffer\n",
    "\n",
    "#### **üö® Reuse Ratio = 4000√ó (Extreme Overfitting)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cba7fe",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Collection:  100 samples\n",
    "Consumption: 400,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ec9d6",
   "metadata": {},
   "source": [
    "- **D·∫•u hi·ªáu**: Model h·ªçc \"thu·ªôc l√≤ng\" buffer\n",
    "- Ch·∫•t l∆∞·ª£ng policy kh√¥ng c·∫£i thi·ªán\n",
    "- Training loss gi·∫£m nh∆∞ng eval performance kh√¥ng tƒÉng\n",
    "\n",
    "---\n",
    "\n",
    "### **4. C·∫•u h√¨nh c·ªßa b·∫°n: Reuse = 32√ó**\n",
    "\n",
    "#### **T√°i t√≠nh to√°n ch√≠nh x√°c:**\n",
    "\n",
    "**M·ªói epoch:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58618554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection:\n",
    "new_transitions = 800 episodes √ó 128 steps = 102,400 transitions\n",
    "\n",
    "# Data consumption:\n",
    "samples_per_update = 512 (batch_size)\n",
    "updates_per_epoch = 256 updates √ó 25 cycles = 6,400 updates\n",
    "total_samples = 512 √ó 6,400 = 3,276,800 samples\n",
    "\n",
    "# Reuse ratio:\n",
    "reuse = 3,276,800 / 102,400 = 32√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ede76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **‚úÖ ƒê√°nh gi√°: MODERATE - ACCEPTABLE**\n",
    "\n",
    "**32√ó reuse l√† m·ª©c ƒë·ªô cao nh∆∞ng v·∫´n ch·∫•p nh·∫≠n ƒë∆∞·ª£c cho off-policy RL:**\n",
    "\n",
    "| Metric | Value | Status |\n",
    "|--------|-------|--------|\n",
    "| Reuse Ratio | **32√ó** | üü° Moderate-High |\n",
    "| Typical SAC | 10-50√ó | ‚úÖ In range |\n",
    "| Risk Level | Medium | ‚ö†Ô∏è Monitor needed |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. T·∫°i sao Reuse cao KH√îNG ph·∫£i l√∫c n√†o c≈©ng x·∫•u?**\n",
    "\n",
    "#### **üéØ ∆Øu ƒëi·ªÉm c·ªßa High Reuse (v·ªõi off-policy RL):**\n",
    "\n",
    "**A. Sample Efficiency cao h∆°n:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac51b2d",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "On-policy (PPO):     1M samples ‚Üí 1M training steps\n",
    "Off-policy (SAC):    100K samples ‚Üí 3.2M training steps (32√ó reuse)\n",
    "                     ‚Üí H·ªçc nhanh h∆°n 32√ó v·ªõi c√πng l∆∞·ª£ng sim data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf44a1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**B. ·ªîn ƒë·ªãnh h∆°n v·ªõi diverse buffer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20,000 trajectories\n",
    "‚Üí Pool size = 20,000 √ó 128 = 2.56M transitions\n",
    "\n",
    "# Random sampling t·ª´ 2.56M transitions\n",
    "# ‚Üí M·ªói update th·∫•y data kh√°c nhau\n",
    "# ‚Üí Gi·∫£m overfitting d√π reuse cao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430f1fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**C. HER augmentation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_p = 0.8\n",
    "‚Üí 80% samples ƒë∆∞·ª£c relabel v·ªõi hindsight goals\n",
    "‚Üí TƒÉng diversity hi·ªáu d·ª•ng\n",
    "‚Üí Effective reuse th·∫•p h∆°n nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496be7b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **‚ö†Ô∏è R·ªßi ro c·ªßa High Reuse:**\n",
    "\n",
    "**A. Q-value overestimation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027ce36",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Critic h·ªçc qu√° fit v·ªõi buffer\n",
    "‚Üí Q-values biased\n",
    "‚Üí Policy exploitation sai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8917c86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**B. Slow adaptation to new data:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e83972",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Model \"√°m ·∫£nh\" v·ªõi old data\n",
    "‚Üí H·ªçc ch·∫≠m t·ª´ new experience\n",
    "‚Üí Plateau performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c6df1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**C. Overfitting symptoms:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9302e72",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Training loss ‚Üì‚Üì‚Üì (gi·∫£m r·∫•t nhanh)\n",
    "Eval performance ‚Üë‚Üì‚Üë (kh√¥ng ·ªïn ƒë·ªãnh)\n",
    "Train-eval gap ‚Üë‚Üë‚Üë (ch√™nh l·ªách l·ªõn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96fef0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Monitoring & Debugging Reuse Issues:**\n",
    "\n",
    "#### **üìä Metrics c·∫ßn theo d√µi:**\n",
    "\n",
    "**A. Training metrics:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trong TensorBoard/logs:\n",
    "- actor_loss: N√™n gi·∫£m ƒë·ªÅu\n",
    "- critic_loss: N√™n gi·∫£m ƒë·ªÅu, kh√¥ng spike\n",
    "- approx_ent: Entropy kh√¥ng n√™n drop qu√° nhanh\n",
    "- ent_coef: Alpha t·ª± ƒëi·ªÅu ch·ªânh, quan s√°t trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8c760",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**B. Evaluation metrics:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "- eps_reward: Ph·∫£i tƒÉng theo epochs\n",
    "- goal_achieved: Success rate tƒÉng\n",
    "- distance: Error gi·∫£m d·∫ßn\n",
    "- eval_value: Q-estimate n√™n t∆∞∆°ng quan v·ªõi reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c35763",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**C. Buffer metrics:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trong logs:\n",
    "- Buffer length: TƒÉng d·∫ßn ƒë·∫øn max_length\n",
    "- Sampling distribution: Uniform vs priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e668587",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **üîç D·∫•u hi·ªáu overfitting:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10838e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD patterns:\n",
    "epoch 10: train_loss=0.05, eval_reward=150\n",
    "epoch 20: train_loss=0.01, eval_reward=160  # Loss gi·∫£m nhanh\n",
    "epoch 30: train_loss=0.005, eval_reward=155 # Reward kh√¥ng tƒÉng/gi·∫£m\n",
    "epoch 40: train_loss=0.001, eval_reward=140 # Reward gi·∫£m!\n",
    "\n",
    "# GOOD patterns:\n",
    "epoch 10: train_loss=0.05, eval_reward=150\n",
    "epoch 20: train_loss=0.03, eval_reward=180  # C·∫£ 2 c·∫£i thi·ªán\n",
    "epoch 30: train_loss=0.02, eval_reward=210\n",
    "epoch 40: train_loss=0.015, eval_reward=235"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c13ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Gi·∫£i ph√°p ƒëi·ªÅu ch·ªânh Reuse Ratio:**\n",
    "\n",
    "#### **üéõÔ∏è Option 1: Gi·∫£m Reuse (tƒÉng collection)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TƒÉng s·ªë cycles = tƒÉng episodes collected\n",
    "learner.run(\n",
    "    num_cycles=1600,  # 800 ‚Üí 1600\n",
    "    num_updates=256,\n",
    "    batch_size=512,\n",
    ")\n",
    "# ‚Üí Reuse = 3.28M / 204,800 = 16√ó (gi·∫£m m·ªôt n·ª≠a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c0918",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Pros:** Gi·∫£m overfitting, diverse data\n",
    "**Cons:** Ch·∫≠m h∆°n, t·ªën simulation time\n",
    "\n",
    "#### **üéõÔ∏è Option 2: Gi·∫£m Reuse (gi·∫£m updates)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.run(\n",
    "    num_cycles=800,\n",
    "    num_updates=128,  # 256 ‚Üí 128\n",
    "    batch_size=512,\n",
    ")\n",
    "# ‚Üí Reuse = 1.64M / 102,400 = 16√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374bd02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Pros:** Nhanh h∆°n Option 1, √≠t overfitting\n",
    "**Cons:** H·ªçc ch·∫≠m h∆°n, c·∫ßn nhi·ªÅu epochs\n",
    "\n",
    "#### **üéõÔ∏è Option 3: Gi·∫£m Reuse (gi·∫£m batch size)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.run(\n",
    "    num_cycles=800,\n",
    "    num_updates=256,\n",
    "    batch_size=256,  # 512 ‚Üí 256\n",
    ")\n",
    "# ‚Üí Reuse = 1.64M / 102,400 = 16√ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3304de",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Pros:** Gi·∫£m memory, ƒë∆°n gi·∫£n\n",
    "**Cons:** Gradient noise cao h∆°n, √≠t stable\n",
    "\n",
    "#### **üéõÔ∏è Option 4: TƒÉng Buffer (gi·∫£m % reuse)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = RHERMemory(\n",
    "    max_length=40000,  # 20,000 ‚Üí 40,000\n",
    "    ...\n",
    ")\n",
    "# Reuse v·∫´n 32√ó, nh∆∞ng sample t·ª´ pool l·ªõn g·∫•p ƒë√¥i\n",
    "# ‚Üí Diversity tƒÉng, overfitting gi·∫£m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22287c4b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Pros:** Diversity cao, kh√¥ng ·∫£nh h∆∞·ªüng training speed\n",
    "**Cons:** T·ªën VRAM (~3-4GB thay v√¨ ~2GB)\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Khuy·∫øn ngh·ªã cho config c·ªßa b·∫°n:**\n",
    "\n",
    "#### **‚úÖ GI·ªÆ NGUY√äN (32√ó reuse) n·∫øu:**\n",
    "- Training loss v√† eval reward ƒë·ªÅu c·∫£i thi·ªán ƒë·ªÅu ƒë·∫∑n\n",
    "- Success rate tƒÉng theo epochs\n",
    "- Kh√¥ng th·∫•y d·∫•u hi·ªáu overfitting (train-eval gap kh√¥ng tƒÉng)\n",
    "- Mu·ªën training nhanh\n",
    "\n",
    "#### **‚ö†Ô∏è ƒêI·ªÄU CH·ªàNH n·∫øu th·∫•y:**\n",
    "\n",
    "**Scenario A: Performance plateau sau 50-100 epochs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58422a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚Üí Gi·∫£m updates/cycle\n",
    "num_updates=128  # thay v√¨ 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadc9e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Scenario B: Train loss gi·∫£m nh∆∞ng eval kh√¥ng c·∫£i thi·ªán**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46153ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚Üí TƒÉng collection\n",
    "num_cycles=1600  # thay v√¨ 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79225c10",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Scenario C: VRAM c√≤n d∆∞ (>4GB free)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4906e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚Üí TƒÉng buffer size\n",
    "max_length=40000  # thay v√¨ 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586faa16",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **T√≥m t·∫Øt:**\n",
    "\n",
    "| Aspect | Your Config | Assessment |\n",
    "|--------|-------------|------------|\n",
    "| **Reuse Ratio** | **32√ó** | üü° Moderate-High |\n",
    "| **Collection/epoch** | 102,400 transitions | ‚úÖ Good |\n",
    "| **Consumption/epoch** | 3.28M samples | üü° High but ok |\n",
    "| **Buffer size** | 20,000 traj (2.56M) | ‚úÖ Sufficient |\n",
    "| **Risk level** | Medium | ‚ö†Ô∏è Monitor needed |\n",
    "| **Action** | **Start with current, monitor, adjust if needed** | üëç |\n",
    "\n",
    "**Reuse 32√ó l√† m·ª©c h·ª£p l√Ω cho SAC + RHER. Kh√¥ng c·∫ßn ƒëi·ªÅu ch·ªânh ngay, nh∆∞ng theo d√µi training curves ƒë·ªÉ ph√°t hi·ªán overfitting s·ªõm!** üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6f34a",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch 'rollout' (ƒë∆°n gi·∫£n v√† c·ª• th·ªÉ)\n",
    "\n",
    "- Rollout = chu·ªói c√°c t∆∞∆°ng t√°c (transitions) thu ƒë∆∞·ª£c khi ch·∫°y policy trong m√¥i tr∆∞·ªùng: observations, actions, rewards, next-observations, v√† th√¥ng tin li√™n quan (done, infos, achieved/desire goals...).\n",
    "- ·ªû repo n√†y, `generate_rollouts` t·∫°o rollout c√≥ ƒë·ªô d√†i `memory.horizon` (c√≤n `envs.num_envs` l√† s·ªë m√¥i tr∆∞·ªùng song song).\n",
    "\n",
    "### Shapes & l√Ω do c·∫Øt d·ªØ li·ªáu\n",
    "- V·ªõi `num_envs = N`, `horizon = H`, sau `generate_rollouts` ta c√≥ (v√≠ d·ª•):\n",
    "  - `rollouts['observation']`: shape `(N, H+1, ...)` ‚Äî bao g·ªìm observation ban ƒë·∫ßu v√† H l·∫ßn next-observation.\n",
    "  - `rollouts['action']`: shape `(N, H, ...)` ‚Äî h√†nh ƒë·ªông t·∫°i m·ªói step.\n",
    "  - `rollouts['achieved_goal']`: shape `(N, H+1, ...)`.\n",
    "  - `rollouts['desired_goal']`: th∆∞·ªùng thu ƒë∆∞·ª£c `(N, H+1, ...)` nh∆∞ng khi store s·∫Ω c·∫Øt th√†nh `(N, H, ...)` t∆∞∆°ng ·ª©ng v·ªõi actions.\n",
    "\n",
    "- T·∫°i sao `store_rollouts` c·∫Øt/move c√°c m·∫£ng:\n",
    "  - Khi l∆∞u transitions c·∫ßn kh·ªõp `(state_t, action_t, reward_t, next_state_{t+1})`.\n",
    "  - Do ƒë√≥ `achieved_goal` ƒë∆∞·ª£c c·∫Øt `[:, 1:]` (b·ªè achieved_goal l√∫c time=0) ƒë·ªÉ kh·ªõp v·ªõi `action_t` ‚Üí ch√≠nh l√† achieved ·ªü next state.\n",
    "  - `desired_goal` ƒë∆∞·ª£c c·∫Øt `[:, :-1]` (b·ªè goal cu·ªëi) ƒë·ªÉ m·ªói `action_t` c√≥ `desired_goal` t∆∞∆°ng ·ª©ng ·ªü th·ªùi ƒëi·ªÉm t.\n",
    "  - N·∫øu c√≥ `meta`, c≈©ng c·∫Øt b·ªè ph·∫ßn ƒë·∫ßu ƒë·ªÉ kh·ªõp t∆∞∆°ng t·ª±.\n",
    "\n",
    "### Vai tr√≤ c·ªßa rollout trong HER / RHER\n",
    "- Rollout l√† ngu·ªìn d·ªØ li·ªáu th√¥; HER thay nh√£n (relabel) goals trong memory khi sample (ƒë·ªïi desired goal th√†nh achieved goal ·ªü m·ªôt th·ªùi ƒëi·ªÉm sau trong c√πng rollout) ƒë·ªÉ h·ªçc t·ª´ tr·∫£i nghi·ªám \"hindsight\".\n",
    "- ·ªû RHER, `r_mix` c√≥ t√°c ƒë·ªông ngay khi t·∫°o rollout: m·ªôt s·ªë step s·∫Ω d√πng goal c·ªßa stage ti·∫øp theo khi ch·ªçn action, v√¨ v·∫≠y rollout ph·∫£n √°nh c·∫£ vi·ªác tr·ªôn goal v√† exploration h∆∞·ªõng m·ª•c ti√™u.\n",
    "\n",
    "### V√≠ d·ª• ng·∫Øn (pick-and-place)\n",
    "- N·∫øu `N=8`, `H=50`: `generate_rollouts` thu `8√ó51` observations, `8√ó50` actions, `8√ó51` achieved_goal. Sau `store_rollouts` l∆∞u v√†o memory c√°c m·∫£ng d·∫°ng `8√ó50` cho transitions `(state_t, action_t, next_state, desired_goal_t, achieved_goal_next, goal_achieved_flag)`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
