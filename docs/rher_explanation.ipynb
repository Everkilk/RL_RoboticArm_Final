{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac0147b",
   "metadata": {},
   "source": [
    "# Giải thích file `drl/learning/rher.py`\n",
    "\n",
    "Tài liệu này tổng hợp các thành phần chính của `drl/learning/rher.py` (phiên bản trong workspace).\n",
    "\n",
    "## Mục đích\n",
    "- `RHER` là một framework huấn luyện dùng Hindsight Experience Replay cho bài toán nhiều giai đoạn (multi-stage). Lớp `RHER` kế thừa `RLFrameWork` và quản lý luồng: tạo rollout, lưu vào bộ nhớ (memory), huấn luyện agent và đánh giá.\n",
    "\n",
    "## Các import chính\n",
    "- `torch`, `numpy`, `tqdm`, `SummaryWriter` — thư viện ML, xử lý tensor và ghi log TensorBoard.\n",
    "- Từ dự án: `RLFrameWork`, `map_structure`, `put_structure`, `groupby_structure`, `nearest_node_value`, `MeanMetrics`, `LOGGER`, `format_time`, `format_tabulate` — các tiện ích và kiểu cơ sở.\n",
    "\n",
    "## Kiến trúc lớp `RHER`\n",
    "- `__init__(envs, agent, memory, compute_metrics=None)`\n",
    "  - Kiểm tra interface của `agent` và `memory` (ví dụ `memory.reward_func`, `memory.num_stages`, `memory.horizon`).\n",
    "  - `compute_metrics` mặc định: kết hợp `goal_achieved` và tỉ lệ reward/horizon để tạo `eval_value` tổng hợp.\n",
    "\n",
    "### Exploration / stage handling\n",
    "- `_get_stages(observations, mt_goals)`\n",
    "  - Gọi `memory.reward_func` cho từng `stage_id` để nhận `goal_achieved` tại mọi stage.\n",
    "  - Tính `stages` hiện tại cho mỗi phần tử trong batch bằng cách xét stage lớn nhất đã đạt.\n",
    "\n",
    "- `_select_action(observations, mt_goals, stages, *, r_mix=0.5)`\n",
    "  - `r_mix` (0..1) là xác suất để \"trộn\" goal: với xác suất `r_mix` mẫu sẽ được chuyển từ stage hiện tại sang stage kế tiếp trước khi chọn action.\n",
    "  - Mục đích: hướng dẫn exploration theo mục tiêu (goal-directed exploration) bằng cách đôi khi cung cấp goal của stage tiếp theo.\n",
    "  - Gọi `agent({'observation':..., 'goal':..., 'task_id': stages}, deterministic=False)` để lấy action non-deterministic.\n",
    "\n",
    "- `select_actions(..., deterministic=False)`\n",
    "  - Nếu không deterministic: chuẩn hoá dữ liệu bằng `agent.format_data`, gọi `_get_stages` và `_select_action`, trả về `(actions_numpy, goal_achieveds_bool)`.\n",
    "  - Nếu deterministic: dùng goal cuối cùng và stage cuối, gọi policy deterministic.\n",
    "\n",
    "### Training\n",
    "- `train(num_updates, batch_size, future_p=0.8, n_steps=1, step_decay=0.7, discounted_factor=0.99, clip_return=None)`\n",
    "  - Nếu dùng prioritized replay thì cập nhật priorities trước.\n",
    "  - Lặp `num_updates` lần: sample từ `memory.sample(...)` (ở đây HER sampling được xử lý trong memory), gọi `agent.update(...)` để cập nhật mạng.\n",
    "  - Trả về `MeanMetrics` chứa các metric trung bình.\n",
    "\n",
    "### Evaluate\n",
    "- `evaluate(num_episodes=10)`\n",
    "  - Chạy envs ở chế độ `eval`, chọn action deterministic, thu thông tin episode (reward, horizon, infos), sử dụng `compute_metrics` để tính `eval_value`.\n",
    "\n",
    "### Run (luồng chính)\n",
    "- `run(...)`\n",
    "  - Tạo thư mục experiment (`make_exp_dir`), có thể `resume` từ checkpoint.\n",
    "  - Với mỗi epoch: lặp cycles, mỗi cycle gọi `generate_rollouts`, `store_rollouts`, sau đó `train` nhiều lần.\n",
    "  - Sau epoch chạy `evaluate` và lưu policy/ckpt nếu cần.\n",
    "\n",
    "### Rollout / Memory\n",
    "- `generate_rollouts(r_mix='auto')`\n",
    "  - Tạo rollout theo `memory.horizon`. Mỗi step: gọi `select_actions` (non-deterministic), thực hiện `envs.step`, thu observations, action, và `goal_achieved`.\n",
    "  - Trả về batch rollouts (cấu trúc đã swap axis) và `info` (mean eps_reward, eps_horizon, ...).\n",
    "\n",
    "- `store_rollouts(rollouts)`\n",
    "  - Chuẩn hoá các chiều mảng (`meta`, `achieved_goal`, `desired_goal`) trước khi gọi `memory.store(rollouts)`.\n",
    "\n",
    "## Về `r_mix` và entropy\n",
    "  - `r_mix`: exploration theo goal (thay đổi input mục tiêu để agent dẫn hướng tới stage tiếp theo).\n",
    "  - Entropy: exploration ở action-space (policy thử nhiều action khác nhau cho cùng goal).\n",
    "\n",
    "## Ví dụ cụ thể về `r_mix` và entropy trong huấn luyện cánh tay robot\n",
    "\n",
    "### Bối cảnh mẫu\n",
    "- Task: pick-and-place chia thành 4 stage: `reach` → `grasp` → `lift` → `place`.\n",
    "- `memory.num_stages = 4`, mỗi rollout có `memory.horizon` bước.\n",
    "- Agent có thể là SAC (policy Gaussian, có hệ số entropy `alpha`) hoặc DDPG/TD3 (thêm noise với std).\n",
    "\n",
    "### Ý tưởng chính (tóm tắt)\n",
    "- `r_mix`: xác suất tại mỗi bước để thay goal hiện tại bằng goal của stage tiếp theo trước khi policy chọn action (goal-guided exploration).\n",
    "- Entropy / noise: điều chỉnh mức độ ngẫu nhiên của policy khi chọn action cho cùng một (state, goal) (action-space exploration).\n",
    "\n",
    "### Cấu hình mẫu và hành vi mong đợi\n",
    "- Cấu hình A — Thận trọng:\n",
    "  - `r_mix = 0.0`, entropy thấp / noise nhỏ (SAC `alpha ≈ 0.01`, DDPG noise std ≈ 0.02).\n",
    "  - Agent học chắc từng stage, ít thử nghiệm goal kế tiếp, ổn định nhưng có thể chậm qua stage.\n",
    "\n",
    "- Cấu hình B — Cân bằng:\n",
    "  - `r_mix = 0.5`, entropy moderate (SAC `alpha ≈ 0.05 - 0.2`, DDPG std ≈ 0.05 - 0.15).\n",
    "  - Một phần rollout hướng tới mục tiêu tiếp theo, policy vẫn thử nhiều hành động — có lợi cho chuyển tiếp giữa stage.\n",
    "\n",
    "- Cấu hình C — Hướng mạnh mục tiêu tiếp theo:\n",
    "  - `r_mix = 1.0`, entropy thấp (nếu entropy cao đồng thời có thể gây bất ổn).\n",
    "  - Agent liên tục nhận goal của stage tiếp theo, đẩy nhanh học chuyển giai đoạn nhưng có rủi ro nếu kỹ năng cơ bản yếu.\n",
    "\n",
    "- Thử nghiệm khám phá rộng:\n",
    "  - `r_mix = 0.8` kết hợp entropy cao (SAC `alpha` lớn hoặc noise std cao) → khám phá rất rộng, khó hội tụ nhưng có thể tìm chuỗi hành động phức tạp.\n",
    "\n",
    "### Ví dụ số cho pick-and-place\n",
    "- Setup: `batch_size=256`, `horizon=50`, `num_stages=4`.\n",
    "- Grid thử: `r_mix` ∈ {0.0, 0.25, 0.5, 0.75, 1.0} × SAC `alpha` ∈ {0.01, 0.05, 0.2} (hoặc DDPG std ∈ {0.02, 0.08, 0.2}).\n",
    "- Theo dõi: success rate per stage, time-to-first-success, average reward, variance. Chạy nhiều seed để kiểm nghiệm.\n",
    "\n",
    "### Cách cấu hình trong code\n",
    "- Truyền `r_mix` khi gọi `run()`:\n",
    "\n",
    "```python\n",
    "# ví dụ\n",
    "rher.run(..., r_mix=0.5, num_updates=50, batch_size=256)\n",
    "```\n",
    "\n",
    "- Cấu hình entropy cho SAC (ví dụ):\n",
    "\n",
    "```python\n",
    "# nếu agent là SAC\n",
    "sac_agent = SACAgent(..., entropy_coef=0.05)\n",
    "```\n",
    "\n",
    "- Cấu hình noise cho DDPG:\n",
    "\n",
    "```python\n",
    "ddpg_agent = DDPGAgent(..., action_noise_std=0.08)\n",
    "```\n",
    "\n",
    "### Lưu ý tuning & thử nghiệm đề xuất\n",
    "- Bắt đầu với `r_mix=0.25-0.5` và entropy moderate (SAC alpha auto hoặc ~0.05). Quan sát: nếu agent không tiến qua stage 1→2 thì tăng `r_mix`; nếu agent thất bại nhiều ở stage hiện tại thì giảm `r_mix` hoặc giảm entropy.\n",
    "- Thử một biến tại một thời điểm (chỉ đổi `r_mix` hoặc chỉ đổi entropy`) để thấy tác động rõ ràng.\n",
    "- Thử nghiệm đề xuất (ngắn):\n",
    "  1. Baseline: `r_mix=0.0`, SAC `alpha=0.05`.\n",
    "  2. Thêm `r_mix=0.5`, giữ `alpha` cố định — so sánh success per stage.\n",
    "  3. Giữ `r_mix=0.5`, thay đổi `alpha` lên/xuống.\n",
    "\n",
    "### Cảnh báo\n",
    "- `generate_rollouts` trong repo có default `r_mix='auto'` nhưng `_select_action` mong `r_mix` là số trong `[0,1]`. Hãy đảm bảo khi gọi `generate_rollouts()` truyền giá trị số thực hoặc sửa `generate_rollouts` để xử lý `'auto'` hợp lý.\n",
    "\n",
    "## Lưu ý thực dụng\n",
    "- `memory.reward_func` phải hỗ trợ `stage_id` (multi-stage reward).\n",
    "- `generate_rollouts` mặc định `r_mix='auto'` nhưng `_select_action` cần `r_mix` là số trong `[0,1]`. Nếu gọi `generate_rollouts()` mà không truyền `r_mix` thì `'auto'` có thể gây lỗi. Nên sửa để xử lý `'auto'` hoặc luôn truyền `r_mix` từ `run()`.\n",
    "- `agent` phải cung cấp `format_data`, `device`, `update`, và chấp nhận input `{'observation','goal','task_id'}`.\n",
    "- `memory` phải implement `store`, `sample`, `reward_func`, `num_stages`, `horizon`, và nếu dùng prioritized replay thì `use_priority` + `update_priorities()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6f34a",
   "metadata": {},
   "source": [
    "## Giải thích 'rollout' (đơn giản và cụ thể)\n",
    "\n",
    "- Rollout = chuỗi các tương tác (transitions) thu được khi chạy policy trong môi trường: observations, actions, rewards, next-observations, và thông tin liên quan (done, infos, achieved/desire goals...).\n",
    "- Ở repo này, `generate_rollouts` tạo rollout có độ dài `memory.horizon` (còn `envs.num_envs` là số môi trường song song).\n",
    "\n",
    "### Shapes & lý do cắt dữ liệu\n",
    "- Với `num_envs = N`, `horizon = H`, sau `generate_rollouts` ta có (ví dụ):\n",
    "  - `rollouts['observation']`: shape `(N, H+1, ...)` — bao gồm observation ban đầu và H lần next-observation.\n",
    "  - `rollouts['action']`: shape `(N, H, ...)` — hành động tại mỗi step.\n",
    "  - `rollouts['achieved_goal']`: shape `(N, H+1, ...)`.\n",
    "  - `rollouts['desired_goal']`: thường thu được `(N, H+1, ...)` nhưng khi store sẽ cắt thành `(N, H, ...)` tương ứng với actions.\n",
    "\n",
    "- Tại sao `store_rollouts` cắt/move các mảng:\n",
    "  - Khi lưu transitions cần khớp `(state_t, action_t, reward_t, next_state_{t+1})`.\n",
    "  - Do đó `achieved_goal` được cắt `[:, 1:]` (bỏ achieved_goal lúc time=0) để khớp với `action_t` → chính là achieved ở next state.\n",
    "  - `desired_goal` được cắt `[:, :-1]` (bỏ goal cuối) để mỗi `action_t` có `desired_goal` tương ứng ở thời điểm t.\n",
    "  - Nếu có `meta`, cũng cắt bỏ phần đầu để khớp tương tự.\n",
    "\n",
    "### Vai trò của rollout trong HER / RHER\n",
    "- Rollout là nguồn dữ liệu thô; HER thay nhãn (relabel) goals trong memory khi sample (đổi desired goal thành achieved goal ở một thời điểm sau trong cùng rollout) để học từ trải nghiệm \"hindsight\".\n",
    "- Ở RHER, `r_mix` có tác động ngay khi tạo rollout: một số step sẽ dùng goal của stage tiếp theo khi chọn action, vì vậy rollout phản ánh cả việc trộn goal và exploration hướng mục tiêu.\n",
    "\n",
    "### Ví dụ ngắn (pick-and-place)\n",
    "- Nếu `N=8`, `H=50`: `generate_rollouts` thu `8×51` observations, `8×50` actions, `8×51` achieved_goal. Sau `store_rollouts` lưu vào memory các mảng dạng `8×50` cho transitions `(state_t, action_t, next_state, desired_goal_t, achieved_goal_next, goal_achieved_flag)`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
